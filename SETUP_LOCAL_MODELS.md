# Setup Local Models (No OpenAI Required!)

## Why Use Local Models?

‚úÖ **No API Costs** - Free forever  
‚úÖ **Complete Privacy** - Data never leaves your network  
‚úÖ **Works Offline** - No internet needed  
‚úÖ **Enterprise Ready** - HIPAA, GDPR compliant  
‚úÖ **Unlimited Usage** - No rate limits  

---

## Quick Setup (15 minutes)

### Step 1: Install Ollama

```bash
# Install Ollama
brew install ollama

# Start Ollama service (in a new terminal)
ollama serve
```

### Step 2: Download a Model

```bash
# Download Llama 3.1 8B (4.7 GB) - Recommended
ollama pull llama3.1:8b

# Or download Mistral 7B (4.1 GB) - Faster
ollama pull mistral:7b

# Test it works
ollama run llama3.1:8b "What is RAG?"
```

### Step 3: Configure the App

```bash
# Edit backend configuration
nano backend/.env
```

Add or change these lines:
```bash
# Set to use local models
USE_LOCAL_MODELS=true

# No OpenAI key needed!
OPENAI_API_KEY=not-needed

# Ollama configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Local embedding model (downloads automatically)
LOCAL_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

### Step 4: Update the App to Use Local Models

```bash
# Edit the main app file
nano backend/app/main.py
```

Change the import at the top:
```python
# Change this:
from app.core.rag_orchestrator import RAG2Orchestrator

# To this:
from app.core.rag_orchestrator_local import RAG2OrchestratorLocal as RAG2Orchestrator
```

### Step 5: Install Additional Dependencies

```bash
cd backend
source venv/bin/activate
pip install langchain-community sentence-transformers
cd ..
```

### Step 6: Start the App

```bash
./start-local.sh
```

---

## That's It!

Your app now runs **100% locally** with:
- ‚úÖ No OpenAI API key needed
- ‚úÖ No internet connection required
- ‚úÖ Zero ongoing costs
- ‚úÖ Complete data privacy

---

## Model Options

### For Testing/Development
**Llama 3.1 8B** (Recommended)
```bash
ollama pull llama3.1:8b
```
- Size: 4.7 GB
- RAM: 8 GB minimum
- Speed: Fast
- Quality: Very good

### For Production
**Llama 3.1 70B** (Best Quality)
```bash
ollama pull llama3.1:70b
```
- Size: 40 GB
- RAM: 64 GB minimum (or GPU with 24GB+ VRAM)
- Speed: Slower
- Quality: Excellent (near GPT-4)

### For Speed
**Mistral 7B**
```bash
ollama pull mistral:7b
```
- Size: 4.1 GB
- RAM: 8 GB minimum
- Speed: Very fast
- Quality: Good

---

## Hardware Requirements

### Minimum (Works on MacBook)
- **CPU**: M1/M2 or Intel i5
- **RAM**: 8 GB
- **Storage**: 10 GB
- **Model**: Llama 3.1 8B or Mistral 7B

### Recommended (Better Performance)
- **CPU**: M2 Pro/Max or Intel i7
- **RAM**: 16 GB
- **Storage**: 50 GB
- **Model**: Llama 3.1 8B

### Production (Best Performance)
- **CPU**: 16+ cores
- **RAM**: 32-64 GB
- **GPU**: NVIDIA RTX 4090 or A100
- **Storage**: 100 GB SSD
- **Model**: Llama 3.1 70B

---

## Performance Comparison

| Setup | Response Time | Quality | Cost/Month | Privacy |
|-------|---------------|---------|------------|---------|
| **OpenAI GPT-4** | 2-5s | Excellent | $500-1500 | ‚ùå Cloud |
| **Local Llama 3.1 8B** | 1-3s | Very Good | $0 | ‚úÖ Local |
| **Local Llama 3.1 70B** | 3-8s | Excellent | $0 | ‚úÖ Local |

---

## Troubleshooting

### Ollama not found?
```bash
# Install Ollama
brew install ollama

# Or download from: https://ollama.ai
```

### Model download fails?
```bash
# Check internet connection
# Try again:
ollama pull llama3.1:8b
```

### App can't connect to Ollama?
```bash
# Make sure Ollama is running
ollama serve

# Check it's working
curl http://localhost:11434/api/tags
```

### Out of memory?
```bash
# Use a smaller model
ollama pull mistral:7b

# Update backend/.env
OLLAMA_MODEL=mistral:7b
```

---

## Switching Between Cloud and Local

You can easily switch between OpenAI and local models:

### Use OpenAI (Cloud)
```bash
# backend/.env
USE_LOCAL_MODELS=false
OPENAI_API_KEY=sk-your-key-here
```

### Use Local Models
```bash
# backend/.env
USE_LOCAL_MODELS=true
OPENAI_API_KEY=not-needed
```

---

## Cost Savings

### OpenAI (3 years)
- Setup: $0
- Monthly: $500-1500
- **Total: $18,000-54,000**

### Local Models (3 years)
- Setup: $3,000 (hardware)
- Monthly: $50 (electricity)
- **Total: $4,800**

**Savings: $13,000-49,000 over 3 years**

---

## Next Steps

1. ‚úÖ Install Ollama
2. ‚úÖ Download a model
3. ‚úÖ Configure the app
4. ‚úÖ Start using it!

**No OpenAI API key needed!** üéâ

---

## Questions?

- **Ollama Docs**: https://ollama.ai/docs
- **Model List**: https://ollama.ai/library
- **Our Guide**: [LOCAL_MODELS_GUIDE.md](LOCAL_MODELS_GUIDE.md)
